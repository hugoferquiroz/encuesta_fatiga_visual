{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c12d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99498c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una carpeta para guardar los gráficos si no existe\n",
    "output_dir = Path(\"../output_graphs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Crea una carpeta para guardar las estadísticas si no existe\n",
    "stats_dir = Path(\"../stats\")\n",
    "stats_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Crea una carpeta para guardar los datos procesados si no existe\n",
    "data_dir = Path(\"../data/2_processed\")\n",
    "data_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f0ee9",
   "metadata": {},
   "source": [
    "# Procesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed85787",
   "metadata": {},
   "source": [
    "## Encuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56adf305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/1_raw/data_upao.xlsx', sheet_name=None)\n",
    "df_cuestionario = df['cuestionario_sin_procesar']\n",
    "\n",
    "# rename the columns to lowercase and snake_case\n",
    "df_cuestionario.columns = [col.lower().replace(\" \", \"_\") for col in df_cuestionario.columns]\n",
    "\n",
    "ls_metrics = [col for col in df_cuestionario.columns if ('unnamed' not in col.lower() and 'trabajador' not in col.lower())]\n",
    "ls_items = ['frecuencia', 'intensidad', 'severidad']\n",
    "\n",
    "ls_columns = []\n",
    "for metric in ls_metrics:\n",
    "    for item in ls_items:\n",
    "        ls_columns.append(f\"{metric}_{item}\")\n",
    "\n",
    "df_cuestionario.columns = ls_columns\n",
    "\n",
    "# elimna la primera fila\n",
    "df_cuestionario = df_cuestionario.iloc[1:, :].reset_index(drop=True)\n",
    "\n",
    "# asegura que todos los datos sean integers by converting each column to numeric (coercing errors to NaN)\n",
    "df_cuestionario = df_cuestionario.apply(pd.to_numeric, errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e4d8fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crea una funcion para verificar que la severidad sea la multiplicacion de frecuencia e intensidad\n",
    "def verify_severity(row):\n",
    "    for metric in ls_metrics:\n",
    "        frecuencia = row[f\"{metric}_frecuencia\"]\n",
    "        intensidad = row[f\"{metric}_intensidad\"]\n",
    "        severidad = row[f\"{metric}_severidad\"]\n",
    "        if pd.isna(frecuencia) or pd.isna(intensidad) or pd.isna(severidad):\n",
    "            continue\n",
    "        if severidad != frecuencia * intensidad:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "df_cuestionario['severity_check'] = df_cuestionario.apply(verify_severity, axis=1)\n",
    "df_cuestionario['severity_check'].sum() == len(df_cuestionario)  # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2039f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula la columna 'sindrome_visual_informatico'\n",
    "df_cuestionario['puntaje_sindrome_visual_informatico'] = df_cuestionario[[f\"{metric}_severidad\" for metric in ls_metrics]].sum(axis=1)\n",
    "df_cuestionario['svi'] = np.where(df_cuestionario['puntaje_sindrome_visual_informatico']>=6, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e5b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encuesta = df['ENCUESTA']\n",
    "df_encuesta.columns = [col.lower().replace(\" \", \"_\") for col in df_encuesta.columns]\n",
    "\n",
    "df_final = pd.concat([df_cuestionario, df_encuesta], axis=1)\n",
    "df_final[df_final.select_dtypes(include=['object']).columns] = (\n",
    "    df_final[df_final.select_dtypes(include=['object']).columns]\n",
    "    .apply(lambda col: col.str.replace(r',\\d+', '', regex=True))\n",
    "    .apply(pd.to_numeric, errors='coerce')\n",
    "    .astype('Int64')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f072395",
   "metadata": {},
   "source": [
    "## Recategorización de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f199532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/1_raw/recategorizacion.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dict_recategorizacion = json.load(f)\n",
    "\n",
    "# convierte las key de dict_recategorizacion de segundo nivel a integer\n",
    "for key in dict_recategorizacion.keys():\n",
    "    dict_recategorizacion[key] = {int(k): v for k, v in dict_recategorizacion[key].items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8e39ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['svi'] = df_final['svi'].map(dict_recategorizacion['svi']).astype(\"category\")\n",
    "\n",
    "df_final['sexo'] = df_final['sexo'].replace({2: 0})\n",
    "df_final['sexo'] = df_final['sexo'].map(dict_recategorizacion['sexo']).astype(\"category\")\n",
    "\n",
    "df_final['estado_civil'] = df_final['estado_civil'].replace({1: 0, 2: 1, 3: 1, 4: 1})\n",
    "df_final['estado_civil'] = df_final['estado_civil'].map(dict_recategorizacion['estado_civil']).astype(\"category\")\n",
    "\n",
    "df_final['ingresos_mensuales'] = df_final['ingresos_mensuales'].replace({1: 0, 2: 0, 3: 0, 4: 1})\n",
    "df_final['ingresos_mensuales'] = df_final['ingresos_mensuales'].map(dict_recategorizacion['ingresos_mensuales']).astype(\"category\")\n",
    "\n",
    "df_final['condiciones_oculares'] = df_final['condiciones_oculares'].replace({1: 1, 2: 0})\n",
    "df_final['condiciones_oculares'] = df_final['condiciones_oculares'].map(dict_recategorizacion['condiciones_oculares']).astype(\"category\")\n",
    "\n",
    "df_final['lentes'] = df_final['lentes'].replace({1: 1, 2: 0})\n",
    "df_final['lentes'] = df_final['lentes'].map(dict_recategorizacion['lentes']).astype(\"category\")\n",
    "\n",
    "df_final['iluminacion'] = df_final['iluminacion'].replace({1: 1, 2: 0})\n",
    "df_final['iluminacion'] = df_final['iluminacion'].map(dict_recategorizacion['iluminacion']).astype(\"category\")\n",
    "\n",
    "df_final['frecuencia_de_pausas'] = df_final['frecuencia_de_pausas'].replace({1: 1, 2: 1, 3: 0})\n",
    "df_final['frecuencia_de_pausas'] = df_final['frecuencia_de_pausas'].map(dict_recategorizacion['frecuencia_de_pausas']).astype(\"category\")\n",
    "\n",
    "df_final['uso_de_dispositivos'] = df_final['uso_de_dispositivos'].replace({1: 0, 2: 0, 3: 1, 4: 1})\n",
    "df_final['uso_de_dispositivos'] = df_final['uso_de_dispositivos'].map(dict_recategorizacion['uso_de_dispositivos']).astype(\"category\")\n",
    "\n",
    "df_final['distancia_hacia_el_monitor'] = df_final['distancia_hacia_el_monitor'].replace({1: 0, 2: 1, 3: 1})\n",
    "df_final['distancia_hacia_el_monitor'] = df_final['distancia_hacia_el_monitor'].map(dict_recategorizacion['distancia_hacia_el_monitor']).astype(\"category\")\n",
    "\n",
    "df_final['tiempo_de_exposicion'] = np.where(df_final['tiempo_de_exposicion']<=12, 0, 1)\n",
    "df_final['tiempo_de_exposicion'] = df_final['tiempo_de_exposicion'].map(dict_recategorizacion['tiempo_de_exposicion']).astype(\"category\")\n",
    "\n",
    "df_final['edad'] = np.where(df_final['edad']<=35, 0, 1)\n",
    "df_final['edad'] = df_final['edad'].map(dict_recategorizacion['edad']).astype(\"category\")\n",
    "\n",
    "df_final['experiencia_radiologia'] = np.where(df_final['experiencia_radiologia']<=3, 0, 1)\n",
    "df_final['experiencia_radiologia'] = df_final['experiencia_radiologia'].map(dict_recategorizacion['experiencia_radiologia']).astype(\"category\")\n",
    "\n",
    "df_final['duracion_de_jornada'] = np.where(df_final['duracion_de_jornada']<=10, 0, 1)\n",
    "df_final['duracion_de_jornada'] = df_final['duracion_de_jornada'].map(dict_recategorizacion['duracion_de_jornada']).astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8470e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"../data/2_processed/df_final_labeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082c45b",
   "metadata": {},
   "source": [
    "## Diccionario de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26925e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openpyxl\n",
    "\n",
    "# # Cargar el archivo\n",
    "# file_path = \"../data/1_raw/data_upao.xlsx\"\n",
    "# workbook = openpyxl.load_workbook(file_path)\n",
    "\n",
    "# # Revisar si existe la hoja \"ENCUESTA\"\n",
    "# if \"ENCUESTA\" in workbook.sheetnames:\n",
    "#     sheet = workbook[\"ENCUESTA\"]\n",
    "#     comments = []\n",
    "#     for row in sheet.iter_rows():\n",
    "#         for cell in row:\n",
    "#             if cell.comment:\n",
    "#                 comments.append({\n",
    "#                     \"celda\": cell.coordinate,\n",
    "#                     \"comentario\": cell.comment.text\n",
    "#                 })\n",
    "# else:\n",
    "#     comments = None\n",
    "\n",
    "# # Extraer los encabezados y sus comentarios, luego armar un diccionario limpio\n",
    "# sheet = workbook[\"ENCUESTA\"]\n",
    "\n",
    "# # Función para pasar encabezado a camel_case\n",
    "# def to_camel_case(s: str) -> str:\n",
    "#     s = s.strip().lower().replace(\" \", \"_\")\n",
    "#     return s\n",
    "\n",
    "# # Diccionario final\n",
    "# categorias_encuesta = {}\n",
    "\n",
    "# for item in comments:\n",
    "#     celda = item[\"celda\"]\n",
    "#     comentario = item[\"comentario\"]\n",
    "    \n",
    "#     # Encabezado en esa celda\n",
    "#     encabezado = sheet[celda].value\n",
    "#     if encabezado is None:\n",
    "#         continue\n",
    "    \n",
    "#     key = to_camel_case(encabezado)\n",
    "    \n",
    "#     # Parsear categorías del comentario (después del \"JORGE:\")\n",
    "#     categorias = {}\n",
    "#     for linea in comentario.split(\"\\n\"):\n",
    "#         if \":\" in linea and linea.strip()[0].isdigit():\n",
    "#             num, texto = linea.split(\":\", 1)\n",
    "#             categorias[int(num.strip())] = texto.strip().lower()\n",
    "    \n",
    "#     if categorias:\n",
    "#         categorias_encuesta[key] = categorias\n",
    "\n",
    "# with open(data_dir / 'categorias_encuesta.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(categorias_encuesta, f, indent=4, ensure_ascii=False)\n",
    "# categorias_encuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded31246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_camel_case(s: str) -> str:\n",
    "#     parts = s.strip().split()\n",
    "#     return parts[0].lower() + ''.join(word.capitalize() for word in parts[1:]) if parts else s\n",
    "\n",
    "# def parse_mapping(mapping_str: str) -> dict:\n",
    "#     # Busca pares del tipo 'texto=número' (los delimitadores pueden ser espacios o comas)\n",
    "#     pairs = re.findall(r'([^=,\\n]+)\\s*=\\s*([<>]?\\s*\\d+(?:-\\d+)?|\\d+)', mapping_str)\n",
    "#     if not pairs:\n",
    "#         return {}\n",
    "#     mapping = {}\n",
    "#     for text, num in pairs:\n",
    "#         try:\n",
    "#             # Extrae el dígito (en caso de rangos o símbolos se limpia la cadena)\n",
    "#             num_clean = re.sub(r'[^\\d]', '', num)\n",
    "#             key_int = int(num_clean)\n",
    "#         except Exception:\n",
    "#             continue\n",
    "#         mapping[key_int] = text.strip().lower()\n",
    "#     return mapping\n",
    "\n",
    "# categorias_metricas = {\n",
    "#     \"fatiga_visual\": parse_mapping(\"Si=1 No=0\"),\n",
    "#     \"tiempo_de_exposicion_a_pantallas\": parse_mapping(\"Horas\"),\n",
    "#     \"edad\": parse_mapping(\"Años\"),\n",
    "#     \"sexo\": parse_mapping(\"Masculino=0 Femenino=1\"),\n",
    "#     \"estado_civil\": parse_mapping(\"Casado=2 Soltero=1 Divorciado=3 o Viudo=4\"),\n",
    "#     \"anios_de_experiencia_profesional\": parse_mapping(\"\"),\n",
    "#     \"ingresos_mensuales\": parse_mapping(\"< 3000=0 3001-5000=1 5001-7000=2 >70000=3\"),\n",
    "#     \"uso_de_correccion_visual\": parse_mapping(\"Si=1 No=0\"),\n",
    "#     \"condiciones_oculares_preexistentes\": parse_mapping(\"Si=0 No=1\"),\n",
    "#     \"pausas_durante_el_trabajo\": parse_mapping(\"Frecuente=0, Ocasional=1, Nulo=2\"),\n",
    "#     \"iluminacion_del_entorno_de_trabajo\": parse_mapping(\"adecuada=0, inadecuada=1\"),\n",
    "#     \"duracion_de_la_jornada_laboral\": parse_mapping(\"\"),\n",
    "#     \"distancia_promedio_al_monitor_mientras_trabaja\": parse_mapping(\"< 30 cm =0 30-60 cm =1 > 60 cm =2\"),\n",
    "#     \"uso_de_dispositivos_moviles_fuera_del_trabajo\": parse_mapping(\"Si=1 No=0\"),\n",
    "# }\n",
    "\n",
    "# with open(data_dir / 'data_categorias_metricas.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(categorias_metricas, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e3a6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # une con categorias_encuesta en un solo diccionario con el nombre todas_categorias\n",
    "# todas_categorias = {**categorias_metricas, **categorias_encuesta}\n",
    "# # exportar a json\n",
    "# with open(data_dir / 'todas_categorias.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(todas_categorias, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6233d",
   "metadata": {},
   "source": [
    "# Análisis descriptivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a284ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.describe(include='all').T.to_csv(stats_dir / 'descriptive_statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52a6ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea una paleta de colores personalizada con el nombre medical_palette que tenga sea sobrio y para una audiencia médica\n",
    "medical_palette = sns.color_palette(\"Set2\")\n",
    "sns.set_palette(medical_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6293368",
   "metadata": {},
   "source": [
    "## Univariado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d25d17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpeta para gráficos univariados\n",
    "univariate_output_dir = output_dir / \"univariate\"\n",
    "univariate_output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c950ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CATEGORIES = 50\n",
    "cat_cols = [\n",
    "    c for c in df_final.columns\n",
    "    if (\n",
    "        df_final[c].dtype == 'object'\n",
    "        or df_final[c].dtype.name == 'category'\n",
    "        or df_final[c].nunique(dropna=False) < 10\n",
    "    )\n",
    "    and df_final[c].nunique(dropna=False) <= MAX_CATEGORIES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b23a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    # Convertir la columna a string, aprovechando que ya es de tipo category o int\n",
    "    ser = df_final[col].astype(str).fillna('NA')\n",
    "\n",
    "    counts = ser.value_counts(dropna=False).sort_values(ascending=False)\n",
    "    df_plot = counts.reset_index()\n",
    "    df_plot.columns = ['category', 'count']\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    pal = medical_palette[:len(df_plot)]  # paleta con la longitud exacta\n",
    "    ax = sns.barplot(data=df_plot, x='category', y='count', hue='category', palette=pal, dodge=False)\n",
    "    # eliminar la leyenda si existe\n",
    "    if ax.get_legend() is not None:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    plt.title(f'Frecuencias de {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(univariate_output_dir / f\"bar_{col}.png\"), bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fed3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar tabla de estadísticas descriptivas para variables categóricas\n",
    "desc_cat = df_final[cat_cols].describe(include='all').T\n",
    "desc_cat.to_csv(stats_dir / 'univariate_categorical_statistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34b4b0",
   "metadata": {},
   "source": [
    "## Análisis bivariado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0906fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos bivariados usando 'svi' como variable dependiente (target)\n",
    "target = 'svi'\n",
    "num_hue = df_final[target].nunique()\n",
    "palette_used = medical_palette[:num_hue]\n",
    "\n",
    "# Crear carpeta para gráficos bivariados\n",
    "bivariate_output_dir = output_dir / \"bivariate\"\n",
    "bivariate_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Seleccionar las columnas categóricas (excluyendo el target) ya definidas como 'category' o 'object'\n",
    "cat_cols_bi = [\n",
    "    c for c in df_final.columns \n",
    "    if c != target and (isinstance(df_final[c].dtype, pd.CategoricalDtype) or df_final[c].dtype == 'object')\n",
    "]\n",
    "\n",
    "for col in cat_cols_bi:\n",
    "    # Gráfico de barras (countplot) usando catplot\n",
    "    g_bar = sns.catplot(\n",
    "        data=df_final, x=col, hue=target, kind='count', \n",
    "        palette=palette_used, height=4, aspect=1.5\n",
    "    )\n",
    "    g_bar.fig.suptitle(f'Distribución de SVI por {col} (Bar)')\n",
    "    g_bar.set_axis_labels(col, 'Frecuencia')\n",
    "    # Ajustar la leyenda a la derecha\n",
    "    g_bar.fig.subplots_adjust(right=0.8)\n",
    "    if g_bar._legend is not None:\n",
    "        g_bar._legend.set_bbox_to_anchor((1.05, 0.5))\n",
    "    g_bar.fig.tight_layout()\n",
    "    g_bar.savefig(str(bivariate_output_dir / f\"bivariate_{col}_bar.png\"))\n",
    "    plt.close(g_bar.fig)\n",
    "    \n",
    "    # Si target es de tipo category o boolean, mapeamos a códigos numéricos.\n",
    "    if isinstance(df_final[target].dtype, pd.CategoricalDtype):\n",
    "        target_num = df_final[target].cat.codes\n",
    "    elif df_final[target].dtype == 'bool':\n",
    "        target_num = df_final[target].astype(int)\n",
    "    else:\n",
    "        try:\n",
    "            target_num = pd.to_numeric(df_final[target])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Crear un DataFrame temporal para incluir la versión numérica del target\n",
    "    df_plot_temp = df_final.copy()\n",
    "    df_plot_temp['_target_num'] = target_num\n",
    "\n",
    "    # Gráfico violin: asignar hue=col y legend=False para evitar el warning\n",
    "    g_violin = sns.catplot(\n",
    "        data=df_plot_temp, x=col, y='_target_num', kind='violin',\n",
    "        hue=col, palette=palette_used, legend=False, height=4, aspect=1.5\n",
    "    )\n",
    "    g_violin.fig.suptitle(f'Distribución de SVI numérico por {col} (Violin)')\n",
    "    g_violin.set_axis_labels(col, target)\n",
    "    g_violin.fig.tight_layout()\n",
    "    g_violin.savefig(str(bivariate_output_dir / f\"bivariate_{col}_violin.png\"))\n",
    "    plt.close(g_violin.fig)\n",
    "\n",
    "    # Gráfico de puntos: asignar hue=col y legend=False para evitar el warning\n",
    "    g_point = sns.catplot(\n",
    "        data=df_plot_temp, x=col, y='_target_num', kind='point',\n",
    "        hue=col, palette=palette_used, legend=False, height=4, aspect=1.5\n",
    "    )\n",
    "    g_point.fig.suptitle(f'Media de SVI por {col} (Point)')\n",
    "    g_point.set_axis_labels(col, target)\n",
    "    g_point.fig.tight_layout()\n",
    "    g_point.savefig(str(bivariate_output_dir / f\"bivariate_{col}_point.png\"))\n",
    "    plt.close(g_point.fig)\n",
    "    \n",
    "    # Eliminar la columna temporal\n",
    "    df_plot_temp.drop(columns=['_target_num'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63cbc5d",
   "metadata": {},
   "source": [
    "# Análisis inferencial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07607a4f",
   "metadata": {},
   "source": [
    "## Alpha de Cronbach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad85324",
   "metadata": {},
   "source": [
    "Mide la consistencia interna de un conjunto de ítems o preguntas que buscan medir la misma característica o constructo:\n",
    "\n",
    "\n",
    "- $\\alpha < 0.6 \\quad \\rightarrow \\quad \\text{baja consistencia (poco confiable).}$\n",
    "\n",
    "- $0.6 \\leq \\alpha < 0.7 \\quad \\rightarrow \\quad \\text{aceptable.}$\n",
    "\n",
    "- $0.7 \\leq \\alpha < 0.8 \\quad \\rightarrow \\quad \\text{buena.}$\n",
    "\n",
    "- $0.8 \\leq \\alpha \\leq 0.9 \\quad \\rightarrow \\quad \\text{muy buena.}$\n",
    "\n",
    "- $\\alpha > 0.9 \\quad \\rightarrow \\quad \\text{excelente, pero puede indicar redundancia (ítems demasiado parecidos).}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ce2b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha de cronbach para las métricas de fatiga visual\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations\n",
    "\n",
    "def cronbach_alpha(df_items):\n",
    "    # Número de ítems\n",
    "    k = df_items.shape[1]\n",
    "    \n",
    "    # Varianza de cada ítem\n",
    "    item_variances = df_items.var(axis=0, ddof=1)\n",
    "    \n",
    "    # Varianza total del test (suma de los ítems)\n",
    "    total_variance = df_items.sum(axis=1).var(ddof=1)\n",
    "    \n",
    "    # Cálculo del alpha de Cronbach\n",
    "    if total_variance == 0:\n",
    "        return np.nan  # Evitar división por cero\n",
    "    alpha = (k / (k - 1)) * (1 - item_variances.sum() / total_variance)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15049063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cronbach's alpha para frecuencia: 0.843 (muy buena)\n",
      "Cronbach's alpha para intensidad: 0.985 (excelente, pero puede indicar redundancia)\n",
      "Cronbach's alpha para severidad: 0.726 (buena)\n"
     ]
    }
   ],
   "source": [
    "def categorize_alpha(alpha):\n",
    "    if np.isnan(alpha):\n",
    "        return \"no definido\"\n",
    "    if alpha < 0.6:\n",
    "        return \"baja consistencia\"\n",
    "    elif alpha < 0.7:\n",
    "        return \"aceptable\"\n",
    "    elif alpha < 0.8:\n",
    "        return \"buena\"\n",
    "    elif alpha <= 0.9:\n",
    "        return \"muy buena\"\n",
    "    else:\n",
    "        return \"excelente, pero puede indicar redundancia\"\n",
    "\n",
    "# Compute Cronbach's alpha for frequency, intensity and severity items\n",
    "df_fatiga_freq = df_final[[f\"{metric}_frecuencia\" for metric in ls_metrics]]\n",
    "alpha_freq = cronbach_alpha(df_fatiga_freq)\n",
    "\n",
    "df_fatiga_int = df_final[[f\"{metric}_intensidad\" for metric in ls_metrics]]\n",
    "alpha_int = cronbach_alpha(df_fatiga_int)\n",
    "\n",
    "df_fatiga_sev = df_final[[f\"{metric}_severidad\" for metric in ls_metrics]]\n",
    "alpha_sev = cronbach_alpha(df_fatiga_sev)\n",
    "\n",
    "print(\"Cronbach's alpha para frecuencia: {:.3f} ({})\".format(alpha_freq, categorize_alpha(alpha_freq)))\n",
    "print(\"Cronbach's alpha para intensidad: {:.3f} ({})\".format(alpha_int, categorize_alpha(alpha_int)))\n",
    "print(\"Cronbach's alpha para severidad: {:.3f} ({})\".format(alpha_sev, categorize_alpha(alpha_sev)))\n",
    "\n",
    "# Exportar los resultados al archivo de estadísticas como csv\n",
    "alpha_results = pd.DataFrame({\n",
    "    'Métrica': ['Frecuencia', 'Intensidad', 'Severidad'],\n",
    "    'Cronbach_Alpha': [alpha_freq, alpha_int, alpha_sev],\n",
    "    'Categoría': [categorize_alpha(alpha_freq), categorize_alpha(alpha_int), categorize_alpha(alpha_sev)]\n",
    "})\n",
    "alpha_results.to_csv(stats_dir / 'cronbach_alpha_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24368e",
   "metadata": {},
   "source": [
    "## Test de medias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2724f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea una copia del df_final pero convierte las categorías a sus valores numéricos\n",
    "df_numeric = df_final.copy()\n",
    "for col in df_numeric.select_dtypes(include=['category', 'object']).columns:\n",
    "    if isinstance(df_numeric[col].dtype, pd.CategoricalDtype):\n",
    "        df_numeric[col] = df_numeric[col].cat.codes.replace(-1, np.nan)  # -1 es para NaN en categorías\n",
    "    else:\n",
    "        try:\n",
    "            df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# exportamos a csv \n",
    "df_numeric.to_csv(\"../data/2_processed/df_final_numeric.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91acba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\encuesta_fatiga_visual\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:579: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "d:\\encuesta_fatiga_visual\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:579: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "d:\\encuesta_fatiga_visual\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:579: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "d:\\encuesta_fatiga_visual\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:579: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "d:\\encuesta_fatiga_visual\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:579: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "d:\\encuesta_fatiga_visual\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:579: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "d:\\encuesta_fatiga_visual\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:579: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "d:\\encuesta_fatiga_visual\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:579: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Realiza un test de medias entre los grupos con y sin SVI \n",
    "# para todas las columnas de df_numeric excepto el target y el score (puntaje_sindrome_visual_informatico)\n",
    "from scipy.stats import mannwhitneyu, ttest_ind, chi2_contingency, fisher_exact\n",
    "\n",
    "target = 'svi'\n",
    "score_col = 'puntaje_sindrome_visual_informatico'\n",
    "test_columns = [col for col in df_numeric.columns if col not in [target, score_col]]\n",
    "\n",
    "results_mannwhitney = []\n",
    "results_ttest = []\n",
    "for col in test_columns:\n",
    "    # Solo se prueban columnas numéricas\n",
    "    if not pd.api.types.is_numeric_dtype(df_numeric[col]):\n",
    "        continue\n",
    "    group_svi = df_numeric[df_numeric[target] == 1][col].dropna()\n",
    "    group_no_svi = df_numeric[df_numeric[target] == 0][col].dropna()\n",
    "    \n",
    "    # Mann–Whitney test\n",
    "    if len(group_svi) > 0 and len(group_no_svi) > 0:\n",
    "        try:\n",
    "            u_stat, p_value = mannwhitneyu(group_svi, group_no_svi, alternative='two-sided')\n",
    "        except Exception:\n",
    "            u_stat, p_value = np.nan, np.nan\n",
    "    else:\n",
    "        u_stat, p_value = np.nan, np.nan\n",
    "\n",
    "    results_mannwhitney.append({\n",
    "        'metric': col,\n",
    "        'mean_svi': group_svi.mean(),\n",
    "        'mean_no_svi': group_no_svi.mean(),\n",
    "        'u_stat': u_stat,\n",
    "        'p_value': p_value,\n",
    "        'differences_significativas': p_value < 0.01 if not np.isnan(p_value) else False\n",
    "    })\n",
    "    \n",
    "    # t-test\n",
    "    if len(group_svi) > 0 and len(group_no_svi) > 0:\n",
    "        try:\n",
    "            t_stat, p_val = ttest_ind(group_svi, group_no_svi, nan_policy='omit')\n",
    "        except Exception:\n",
    "            t_stat, p_val = np.nan, np.nan\n",
    "    else:\n",
    "        t_stat, p_val = np.nan, np.nan\n",
    "\n",
    "    results_ttest.append({\n",
    "        'metric': col,\n",
    "        'mean_svi': group_svi.mean(),\n",
    "        'mean_no_svi': group_no_svi.mean(),\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_val,\n",
    "        'differences_significativas': p_val < 0.05 if not np.isnan(p_val) else False\n",
    "    })\n",
    "\n",
    "df_mannwhitney = pd.DataFrame(results_mannwhitney)\n",
    "df_mannwhitney.to_csv(stats_dir / 'mannwhitney_svi_vs_no_svi.csv', index=False)\n",
    "\n",
    "df_ttest = pd.DataFrame(results_ttest)\n",
    "df_ttest.to_csv(stats_dir / 'ttest_svi_vs_no_svi.csv', index=False)\n",
    "\n",
    "# ---------------------------\n",
    "# Test de independencia para variables categóricas: Chi-cuadrado o Fisher\n",
    "cat_test_columns = [col for col in df_final.columns \n",
    "                    if ((df_final[col].dtype == 'object') or (str(df_final[col].dtype).startswith('category')))\n",
    "                    and col not in [target, score_col]]\n",
    "\n",
    "results_chi = []\n",
    "for col in cat_test_columns:\n",
    "    # Construir la tabla de contingencia\n",
    "    table = pd.crosstab(df_final[col], df_final[target])\n",
    "    if table.empty:\n",
    "        continue\n",
    "    \n",
    "    # Si la tabla es 2x2 usamos el test exacto de Fisher; si no, usamos Chi-cuadrado\n",
    "    if table.shape == (2,2):\n",
    "        try:\n",
    "            _, p = fisher_exact(table)\n",
    "            test_name = \"Fisher\"\n",
    "        except Exception:\n",
    "            p = np.nan\n",
    "            test_name = \"Fisher\"\n",
    "    else:\n",
    "        try:\n",
    "            chi2, p, dof, expected = chi2_contingency(table)\n",
    "            test_name = \"Chi-cuadrado\"\n",
    "        except Exception:\n",
    "            p = np.nan\n",
    "            test_name = \"Chi-cuadrado\"\n",
    "    \n",
    "    results_chi.append({\n",
    "        'variable': col,\n",
    "        'test': test_name,\n",
    "        'p_value': p,\n",
    "        'differences_significativas': p < 0.05 if not np.isnan(p) else False\n",
    "    })\n",
    "\n",
    "df_chi = pd.DataFrame(results_chi)\n",
    "df_chi.to_csv(stats_dir / 'chi_square_fisher_svi_vs_no_svi.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03a5b0",
   "metadata": {},
   "source": [
    "# Causalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db6274a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos las columnas que tiene los sufijos \"_frecuencia\", \"_intensidad\", \"_severidad\"\n",
    "df_models = df_numeric.drop(columns=[col for col in df_numeric.columns if col.endswith((\"_frecuencia\", \"_intensidad\", \"_severidad\"))])\n",
    "\n",
    "# eliminamos las columnas 'puntaje_sindrome_visual_informatico' y 'severity_check'\n",
    "df_models = df_models.drop(columns=['puntaje_sindrome_visual_informatico', 'severity_check', 'trabajador'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b433506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos un correlograma de sperman, anova y Kendall’s de todas las variables numéricas en df_models\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df_models.corr(method='spearman'), annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, square=True, linewidths=.5)\n",
    "plt.title(\"Correlograma Spearman de Variables Numéricas\")\n",
    "plt.savefig(str(output_dir / \"correlograma_variables_numericas_spearman.png\"), bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df_models.corr(method='kendall'), annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, square=True, linewidths=.5)\n",
    "plt.title(\"Correlograma Kendall de Variables Numéricas\")\n",
    "plt.savefig(str(output_dir / \"correlograma_variables_numericas_kendall.png\"), bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34d8be6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('edad', 'estado_civil'),\n",
       " ('edad', 'experiencia_radiologia'),\n",
       " ('edad', 'ingresos_mensuales'),\n",
       " ('experiencia_radiologia', 'ingresos_mensuales'),\n",
       " ('experiencia_radiologia', 'lentes'),\n",
       " ('condiciones_oculares', 'lentes')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminamos las columnas que tengan correlacion mayor a 0.6 y menor a 0.6\n",
    "corr_matrix = df_models.corr()\n",
    "high_corr_var = np.where((corr_matrix > 0.6) | (corr_matrix < -0.6))\n",
    "high_corr_var = [(corr_matrix.index[x], corr_matrix.columns[y]) for x, y in zip(*high_corr_var) if x != y and x < y]\n",
    "high_corr_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ec61406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = df_models.drop(columns=['edad', 'experiencia_radiologia', 'condiciones_oculares', 'duracion_de_jornada'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aafba3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar df_models a csv\n",
    "df_models.to_csv(\"../data/2_processed/df_models.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7635e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.read_csv(\"../data/2_processed/df_models.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b81dc",
   "metadata": {},
   "source": [
    "## Regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d2d1c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    svi   R-squared:                       0.127\n",
      "Model:                            OLS   Adj. R-squared:                 -0.013\n",
      "Method:                 Least Squares   F-statistic:                    0.9053\n",
      "Date:                Wed, 01 Oct 2025   Prob (F-statistic):              0.476\n",
      "Time:                        22:10:20   Log-Likelihood:                -19.132\n",
      "No. Observations:                  30   AIC:                             48.26\n",
      "Df Residuals:                      25   BIC:                             55.27\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "const                          0.7101      0.427      1.664      0.109      -0.169       1.589\n",
      "tiempo_de_exposicion           0.1024      0.202      0.508      0.616      -0.313       0.518\n",
      "frecuencia_de_pausas          -0.3031      0.377     -0.803      0.429      -1.080       0.474\n",
      "iluminacion                   -0.0273      0.192     -0.142      0.888      -0.423       0.368\n",
      "distancia_hacia_el_monitor     0.2523      0.198      1.272      0.215      -0.156       0.661\n",
      "==============================================================================\n",
      "Omnibus:                       15.899   Durbin-Watson:                   1.628\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):                3.058\n",
      "Skew:                          -0.239   Prob(JB):                        0.217\n",
      "Kurtosis:                       1.510   Cond. No.                         9.98\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "y = df_models['svi']\n",
    "X = df_models[['tiempo_de_exposicion', 'frecuencia_de_pausas', 'iluminacion', 'distancia_hacia_el_monitor']]\n",
    "X = sm.add_constant(X)  # Agregar constante para el intercepto\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    svi   R-squared:                       0.164\n",
      "Model:                            OLS   Adj. R-squared:                 -0.212\n",
      "Method:                 Least Squares   F-statistic:                    0.4364\n",
      "Date:                Tue, 30 Sep 2025   Prob (F-statistic):              0.899\n",
      "Time:                        22:40:01   Log-Likelihood:                -18.472\n",
      "No. Observations:                  30   AIC:                             56.94\n",
      "Df Residuals:                      20   BIC:                             70.96\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "const                          0.5308      0.550      0.965      0.346      -0.616       1.678\n",
      "sexo                          -0.0697      0.252     -0.276      0.785      -0.596       0.456\n",
      "estado_civil                   0.0539      0.253      0.213      0.834      -0.475       0.583\n",
      "ingresos_mensuales             0.1403      0.264      0.532      0.600      -0.410       0.690\n",
      "lentes                        -0.1296      0.267     -0.486      0.632      -0.686       0.427\n",
      "iluminacion                   -0.0302      0.215     -0.140      0.890      -0.479       0.419\n",
      "tiempo_de_exposicion           0.0565      0.244      0.231      0.820      -0.453       0.566\n",
      "frecuencia_de_pausas          -0.3011      0.449     -0.671      0.510      -1.238       0.635\n",
      "uso_de_dispositivos            0.1819      0.246      0.741      0.467      -0.330       0.694\n",
      "distancia_hacia_el_monitor     0.3245      0.241      1.347      0.193      -0.178       0.827\n",
      "==============================================================================\n",
      "Omnibus:                       10.354   Durbin-Watson:                   1.509\n",
      "Prob(Omnibus):                  0.006   Jarque-Bera (JB):                2.423\n",
      "Skew:                          -0.100   Prob(JB):                        0.298\n",
      "Kurtosis:                       1.622   Cond. No.                         14.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# haz un modelo de regresión lineal que tenga como variable dependiente 'svi' y como variables independientes todas las demás variables numéricas en df_models\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "y = df_models['svi']\n",
    "X = df_models.drop(columns=['svi'])\n",
    "X = sm.add_constant(X)  # Agregar constante para el intercepto\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d7204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c37fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d626e99",
   "metadata": {},
   "source": [
    "## Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee3205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c90957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
